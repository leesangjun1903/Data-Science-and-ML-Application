Information Theory에서 Entropy는 measure 즉 정보량을 측정하는 도구로 "Entropy가 높다는 것 = Uncertainty가 높다는 것 = Information 양이 많다는 것"을 의미합니다.

이런 배경속에서 Mutual Information(MI)는 2개의 R.V.(Random Variable)들 간의 상호의존성(mutual dependence)을 확인하는 지표입니다. 다른 말로 Information Gain, KL-Divergence이라고도 알려져 있습니다. 정확히는 KL-Divergence와는 다릅니다.

# Reference
https://aigong.tistory.com/43
