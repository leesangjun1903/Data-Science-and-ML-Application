# Machine Learning Theory
## Dive into Deep Learning, ASTON ZHANG
Data Manipulation  
Data Preprocessing  
Linear Algebra  
Calculus  
Automatic Differentiation  
Probability and Statistics  
Linear Regression  
Object-Oriented Design for Implementation  
Synthetic Regression Data  
Linear Regression Implementation from Scratch  
Concise Implementation of Linear Regression  
Generalization  
Weight Decay  
Softmax Regression  
The Image Classification Dataset  
The Base Classification Model  
Softmax Regression Implementation from Scratch  
Concise Implementation of Softmax Regression  
Generalization in Classification  
Environment and Distribution Shift  
Multilayer Perceptrons  
Implementation of Multilayer Perceptrons  
Forward Propagation, Backward Propagation, and Computational Graphs  
Numerical Stability and Initialization  
Generalization in Deep Learning  
Dropout  
Predicting House Prices on Kaggle  
Layers and Modules  
Parameter Management  
Parameter Initialization  
Lazy Initialization  
Custom Layers  
File I/O  
GPUs  
From Fully Connected Layers to Convolutions  
Convolutions for Images  
Padding and Stride  
Multiple Input and Multiple Output Channels  
Pooling  
Convolutional Neural Networks (LeNet)  
Deep Convolutional Neural Networks (AlexNet)  
Networks Using Blocks (VGG)  
Network in Network (NiN)  
Multi-Branch Networks (GoogLeNet)  
Batch Normalization  
Residual Networks (ResNet) and ResNeXt  
Densely Connected Networks (DenseNet)  
Designing Convolution Network Architectures  
Working with Sequences  
Converting Raw Text into Sequence Data  
Language Models  
Recurrent Neural Networks  
Recurrent Neural Network Implementation from Scratch  
Concise Implementation of Recurrent Neural Networks  
Backpropagation Through Time  
Long Short-Term Memory (LSTM)  
Gated Recurrent Units (GRU)  
Deep Recurrent Neural Networks  
Bidirectional Recurrent Neural Networks  
Machine Translation and the Dataset  
The Encoder−Decoder Architecture  
Sequence-to-Sequence Learning for Machine Translation  
Beam Search  
Queries, Keys, and Values  
Attention Pooling by Similarity  
Attention Scoring Functions  
The Bahdanau Attention Mechanism  
Multi-Head Attention  
Self-Attention and Positional Encoding  
The Transformer Architecture  
Transformers for Vision  
Large-Scale Pretraining with Transformers  
Optimization and Deep Learning  
Convexity  
Gradient Descent  
Stochastic Gradient Descent  
Minibatch Stochastic Gradient Descent  
Momentum  
Adagrad  
RMSProp  
Adadelta  
Adam  
Learning Rate Scheduling  
Compilers and Interpreters  
Asynchronous Computation  
Automatic Parallelism  
Hardware  
Training on Multiple GPUs  
Concise Implementation for Multiple GPUs  
Parameter Servers  
Image Augmentation  
Fine-Tuning  
Object Detection and Bounding Boxes  
Anchor Boxes  
Multiscale Object Detection  
The Object Detection Dataset  
Single Shot Multibox Detection  
Region-based CNNs (R-CNNs)  
Semantic Segmentation and the Dataset  
Transposed Convolution  
Fully Convolutional Networks  
Neural Style Transfer  
Image Classification (CIFAR-10) on Kaggle  
Dog Breed Identification (ImageNet Dogs) on Kaggle  
Word Embedding (word2vec)  
Approximate Training  
The Dataset for Pretraining Word Embeddings  
Pretraining word2vec  
Word Embedding with Global Vectors (GloVe)  
Subword Embedding  
Word Similarity and Analogy  
Bidirectional Encoder Representations from Transformers (BERT)  
The Dataset for Pretraining BERT  
Pretraining BERT  
Sentiment Analysis and the Dataset  
Sentiment Analysis: Using Recurrent Neural Networks  
Sentiment Analysis: Using Convolutional Neural Networks  
Natural Language Inference and the Dataset  
Natural Language Inference: Using Attention  
Fine-Tuning BERT for Sequence-Level and Token-Level Applications  
Natural Language Inference: Fine-Tuning BERT  
Markov Decision Process (MDP)  
Value Iteration  
Q-Learning  
Introduction to Gaussian Processes  
Gaussian Process Priors  
Gaussian Process Inference  
What Is Hyperparameter Optimization?  
Hyperparameter Optimization API  
Asynchronous Random Search  
Multi-Fidelity Hyperparameter Optimization  
Asynchronous Successive Halving  
Generative Adversarial Networks  
Deep Convolutional Generative Adversarial Networks  
Overview of Recommender Systems 


## UNDERSTANDING MACHINE LEARNING, Shai Shalev-Shwartz
A Gentle Start  
A Formal Learning Model  
Learning via Uniform Convergence  
The Bias-Complexity Tradeoff  
The VC-Dimension  
Nonuniform Learnability  
The Runtime of Learning  
Linear Predictors  
Boosting  
Model Selection and Validation  
Convex Learning Problems  
Regularization and Stability  
Stochastic Gradient Descent  
Support Vector Machines  
Kernel Methods  
Multiclass, Ranking, and Complex Prediction Problems  
Decision Trees  
Nearest Neighbor  
Neural Networks  
Online Learning  
Clustering  
Dimensionality Reduction  
Generative Models  
Feature Selection and Generation  
Rademacher Complexities  
Covering Numbers  
Proof of the Fundamental Theorem of Learning Theory  
Multiclass Learnability  
Compression Bounds  
PAC-Bayes  



## 딥러닝과 수학, 오승상
Introduction to Deep Learning  
Deep Neural Network  
Convolutional Neural Network  
Recurrent Neural Network  
Attention Mechanism  
Auto-Encoder & VAE  
Generative Adversarial Network  
Natural Language Processing  
Graph Neural Network  

## Automated Machine Learning, Frank Hutter
Automated machine learning, also referred to as automated ML or AutoML, is the process of automating the time-consuming, iterative tasks of machine learning model development.  

- Hyperparameter Optimization
- Meta-Learning
- Neural Architecture Search
- Auto-WEKA: Automatic Model Selection and Hyperparameter Optimization in WEKA
- Hyperopt-Sklearn
- Auto-sklearn: Efficient and Robust Automated Machine Learning
- Towards Automatically-Tuned Deep Neural Networks
- TPOT: A Tree-Based Pipeline Optimization Tool for Automating Machine Learning
- The Automatic Statistician
- Analysis of the AutoML Challenge Series 2015–2018 

## cs231n, Fei-Fei Li
Image Classification pipeline  
Loss functions and Optimization  
Backpropagation and Neural Networks  
Training Neural Networks  
Convolutional Neural Networks  
Spatial Localization and Detection  
Understanding and Visualizing Convolutional Neural Networks  
Recurrent Neural Networks  
CNNs in Practice  
Software Packages: Caffe / Torch / Theano / TensorFlow  
Segmentation and Attention  
Videos, Unsupervised Learning  

## Generative Deep Learning, David Foster
Generative Modeling  
Deep Learning  
Variational Autoencoders  
Generative Adversarial Networks  
Autoregressive Models  
Normalizing Flow Models  
Energy-Based Models  
Diffusion Models  
Transformers  
Advanced GANs  
Music Generation  
World Models  
Multimodal Models  
Timeline of Generative AI 

# ETC
Reproducing Kernel Hilbert Space, Mercer’s Theorem, Eigenfunctions, Nystr¨ om Method, and Use of Kernels in Machine Learning: Tutorial and Survey, Benyamin Ghojogh  
An Introduction to Reproducing Kernel Hilbert Spaces and Why They are So Useful, Grace Wahba  
